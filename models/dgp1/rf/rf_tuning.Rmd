---
title: "Random Forest Model Training"
subtitle: "Random forest hyperparameters tuned on full and observed datasets for each case separately"
author: "Amos Okutse"
date: "  `r format(Sys.time(), '%d %B, %Y')` "
header-includes:
- \usepackage{color, colortbl}  % for using table colors
- \definecolor{Gray}{gray}{0.9}  %define the color to be used
- \usepackage{multirow}
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \usepackage{fvextra}
- \usepackage{float}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{float}
- \usepackage{graphicx}
- \usepackage{microtype}
- \usepackage{setspace}
- \usepackage[font=singlespacing]{caption} #can change font here for captions here!!
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines, commandchars=\\\{\}}
#- \onehalfspacing
fontsize: 10pt
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: true
    toc_depth: 4
    number_sections: true
    #keep_md: true
link-citation: yes
colorlinks: yes
linkcolor: blue
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	cache = FALSE,
	message = FALSE,
	warning = FALSE,
	fig.align = 'center',
	fig.pos = 'H',
	dpi = 350,
	tidy.opts = list(width.cutoff = 80, tidy = TRUE)
)
```

```{r, echo=FALSE, include= FALSE}
# function to install missing packages
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE, repos='http://cran.rstudio.com/')
  sapply(pkg, require, character.only = TRUE)
}
packages =c( "tidyverse","knitr", "kableExtra","skimr", "MatchIt", "RItools","optmatch", "ggplot2", "tufte", "tufterhandout", "plotly", "snowfall", "rstan", "gridExtra", "knitr", "gtsummary", "data.table", "GGally", "MASS", "broom", "boot", "foreach", "doParallel", "glmnet", "tidymodels" , "usemodels", "magrittr")
ipak(packages)
```

```{r echo=FALSE, eval=FALSE}
bytes <- file.size("simulation_six_v3.Rmd")
words <- bytes/10
minutes <- words/200
```



## Introduction

Generate the data and see whether the two-step process results in less biased estimates? [We are adjusting for covariates that did not generate the outcome $Y$, at least not directly since the outcome was generated by $Z_i's$ which were not observed]

## Data generating process {#data-process}

The data generating process is derived from Kang and Schaffer (2007) pg. 526. Assume we have a random sample, $i = 1, \cdots, n$ from an infinite population. Outcome variable, $y_i$, $R_i$ is the response indicator corresponding to 1 if $y_i$ is observed and 0 otherwise. $x_i$ is the p-dimensional vector of covariates that may be related to $y_i$ and $R_i$. The population response and non-response rates are denoted by $r^{(1)}=P(R_i=1)$ and $P(R_i = 0)$, respectively, whereas the sample response rates are denoted by $\hat{r}^{(1)}=n^{(1)}/n$ and $\hat{r}^{(0)}=n^{(0)}/n$. For each $i$ suppose that $(z_{i1}, z_{i2}, z_{i3}, z_{i4})^T$ are independently distributed as $N(0, I)$ where $I$ is a $4\times 4$ identity matrix. $y_i's$ are generated as:

\begin{equation}
y_i = 210 + 50A_i + 27.4Z_{i1} + 13.7z_{i2} + 13.7z_{i3} + 13.7z_{i4} +\epsilon_i
(\#eq:eqn-one)
\end{equation}

where $\epsilon \sim N(0, 1)$ and the true propensity scores are defined as:

\begin{equation}
\pi_i = \text{expit}(-z_{i1}+0.5z_{i2}-0.25z_{i3}-0.1z_{i4})
(\#eq:eqn-two)
\end{equation}

The average response rate is assumed as $r^{(1)}=0.5$ and the difference in means is $E(y/R=1)-E(y/R=0)=20$. The correct $\pi$ model in this scenario is a logistic regression model of $R_i$ on the $z_{ij}$'s whereas the correct $y$ model is a linear regression of $y_i$ on the $z_{ij}$'s. Suppose, however, that the covariates actually observed by the data analyst are:

\begin{align*}
x_{i1}&=exp(z_{i1}/2),\\
x_{i2}&=z_{i2}/(1+exp(Z_{i1}))+10, \\
x_{i3}&=(z_{i1}z_{i3}/25+0.6)^3, \\
x_{i4}&=(z_2+z_4+20)^2
\end{align*}

such that logit($\pi_i$) and $E(y_i/x)$ are linear functions of $log(x_{i1}), x_2, x_1^2x_2, 1/log(x_1), x_3/log(x_1), x_4^{1/2}$. We proceed by sampling $n = 1000$ observations as follows:

- Set $A_i = 1$ for $i = 1, \cdots, 500$ and $A_i = 0$ for $i = 501, \cdots, 1000$.

- Draw $z_{i1}, \cdots, z_{i4} \sim N(0, I)$ where $I$ is a $4 \times 4$ identity matrix.

- Calculate $y_i$ as in \@ref(eq:eqn-one).

- Calculate $\text{expit}(-z_{i1}+0.5z_{i2}-0.25z_{i3}-0.1z_{i4})$ 

- Draw $R_i \sim Ber(\pi_i)$

- Denote $R_i = 0$ as those with missing outcome data $y_i$

- Calculate the $x_i's$ from the $z_i's$

```{r}
setwd("C:\\Users\\aokutse\\OneDrive - Brown University\\ThesisResults\\[4]_random_forest\\train_rf")
## set up the simulation
rm(list = ls())
## load the saved single data files
## load the individual data files
load("G:\\Shared drives\\amos\\ThesisResults\\data\\df_one.RData")
load("G:\\Shared drives\\amos\\ThesisResults\\data\\df_two.RData")
load("G:\\Shared drives\\amos\\ThesisResults\\data\\df_three.RData")
load("G:\\Shared drives\\amos\\ThesisResults\\data\\df_four.RData")

## load the saved list data files
##load("G:\\Shared drives\\amos\\ThesisResults\\data\\dsets1.RData")
##load("G:\\Shared drives\\amos\\ThesisResults\\data\\dsets2.RData")
##load("G:\\Shared drives\\amos\\ThesisResults\\data\\dsets3.RData")
##load("G:\\Shared drives\\amos\\ThesisResults\\data\\dsets4.RData")

```



### RANDOM FOREST MODELS

## TUNING ON THE FULL DATASET

#### Full data modelling [case 1 when n = 500 and sd = 1]

- All model parameter tuning are based on the full data.

- Parameters are explored for all possible scenarios on the full data set with 10-fold cross-validation.

- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.


```{r}

## using data with n = 500 and sd = 1 (df_one) 

train <- df_one[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```


```{r fig.cap="Variable importance for the full data case with $n = 500$ and $SD = 1$"}
## variable importance??
final_md <- finalize_model(rf_model, rf_best)
options(scipen = 999)
full_one <- final_md %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = df_one[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
full_one


```




#### Full data modeling [case 2 when n = 500 and sd = 45]


- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.

- Based on this procedure, the best tuning parameters with n = 1000 trees were `min_n = 23` and `mtry = 4` 


```{r}

## using data with n = 500 and sd = 45 (df_two) 

train <- df_two[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```

```{r fig.cap="Variable importance for the full data case with $n = 500$ and $SD = 45$"}
## summarize variable importance
final_md2 <- finalize_model(rf_model, rf_best)
options(scipen = 999)
full_two <- final_md2 %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = df_two[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
full_two
```


#### Full data modeling [case 3 when n = 2000 and sd = 1]


- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.

- Based on this procedure, the best tuning parameters with n = 1000 trees were `min_n = 4` and `mtry = 5` 

```{r}

## using data with n = 2000 and sd = 1 (df_three) 

train <- df_three[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```

```{r fig.cap="Variable importance for the full data case with $n = 2000$ and $SD = 1$"}
## summarize variable importance
final_md3 <- finalize_model(rf_model, rf_best)
options(scipen = 999)
full_three <- final_md3 %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = df_three[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
full_three
```

#### Full data modeling [case 4 when n = 2000 and sd = 45]


- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.

- Based on this procedure, the best tuning parameters with n = 1000 trees were `min_n = 35` and `mtry = 2` 

```{r}
## using data with n = 2000 and sd = 45 (df_three) 

train <- df_four[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best

```


```{r fig.cap="Variable importance for the full data case with $n = 2000$ and $SD = 45$"}
## summarize variable importance
final_md4 <- finalize_model(rf_model, rf_best)
options(scipen = 999)
full_four <- final_md4 %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = df_four[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
full_four

```


## TUNING ON OBSERVED DATA



#### [case 1 when n = 500 and sd = 1]

- All model parameter tuning are based on the full data.

- Parameters are explored for all possible scenarios on the full data set with 10-fold cross-validation.

- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.


```{r}

## using data with n = 500 and sd = 1 (df_one) 

train1 <- base::subset(df_one, R == 1)
train <- train1[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```

```{r fig.cap="Variable importance for the observed data case with $n = 500$ and $SD = 1$"}
## summarize variable importance
final_obs <- finalize_model(rf_model, rf_best)
options(scipen = 999)
obs_one <- final_obs %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = base::subset(df_one, R == 1)[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
obs_one

```




#### [case 2 when n = 500 and sd = 45]


- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.

- Based on this procedure, the best tuning parameters with n = 1000 trees were `min_n = 23` and `mtry = 4` 


```{r}

## using data with n = 500 and sd = 45 (df_two) 

train1 <- base::subset(df_two, R == 1)
train = train[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```


```{r fig.cap="Variable importance for the observed data case with $n = 500$ and $SD = 45$"}

## summarize variable importance
final_obs <- finalize_model(rf_model, rf_best)
options(scipen = 999)
obs_two <- final_obs %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = base::subset(df_two, R == 1)[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
obs_two
```


#### [case 3 when n = 2000 and sd = 1]


- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.

- Based on this procedure, the best tuning parameters with n = 1000 trees were `min_n = 4` and `mtry = 5` 

```{r}

## using data with n = 2000 and sd = 1 (df_three) 

train1 <- base::subset(df_three, R == 1)
train = train1[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```


```{r fig.cap="Variable importance for the observed data case with $n = 2000$ and $SD = 1$"}
## summarize variable importance
final_obs <- finalize_model(rf_model, rf_best)
options(scipen = 999)
obs_three <- final_obs %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = base::subset(df_three, R == 1)[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
obs_three
```

#### [case 4 when n = 2000 and sd = 45]


- We use a space-filling grid design and search parameters across a grid of 25 models using cross-validation.

- Based on this procedure, the best tuning parameters with n = 1000 trees were `min_n = 35` and `mtry = 2` 

```{r}
## using data with n = 2000 and sd = 45 (df_three) 

train1 <- base::subset(df_four, R == 1)
train = train1[, -7]


## create the random forest model object 
rf_model <- rand_forest(trees = 1000, mtry = tune(), min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

## create the workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(y ~ A + x1 + x2 + x3 + x4)

## create the procedure for validating the model
val_set <- validation_split(train, prop = 0.80)

## set up the set of metrics to gather from the models [there is no mse; can't use accuracy too which is for class]
metrics <- metric_set(rmse, rsq) ## rsq=coefficient of determination = 


## create the re-sampling folds for hyper-parameter tuning
set.seed(345)
folds <- vfold_cv(train, v = 10)


## fit re-samples and estimate the hyper parameters
doParallel::registerDoParallel()  ## leverage parallel processing

set.seed(456)
rf_results <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            resamples = folds,
            control = control_grid(save_pred = TRUE), #saving preds allows collecting the metrics
            metrics = metric_set(rmse))

rf_results %>% show_best(metric = "rmse")

## can plot the best models
autoplot(rf_results)


## rf best model based on accuracy
rf_best = rf_results %>% 
  select_best(metric = "rmse") 
rf_best
```


```{r fig.cap="Variable importance for the observed data case with $n = 2000$ and $SD = 45$"}

## summarize variable importance
final_obs <- finalize_model(rf_model, rf_best)
options(scipen = 999)
obs_four <- final_obs %>%
 set_engine("ranger", importance = "impurity") %>%
  fit(y ~ .,
    data = base::subset(df_four, R == 1)[, -7]) %>%
  vip::vip(geom = "col") + theme_minimal()
obs_four

## create a grid of variable importance across all the cases under full data hyper-parameter tuning
jpeg("vip.jpeg", width = 4, height = 4, units = 'in', res = 300)
full_four
dev.off()

```




















